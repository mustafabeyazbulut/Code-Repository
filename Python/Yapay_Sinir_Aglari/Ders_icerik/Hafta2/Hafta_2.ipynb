{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward\n",
        "\n",
        "Pytorch kullanarak, input vektörü \n",
        "\n",
        "[0.1\n",
        "\n",
        "0.3\n",
        "\n",
        "0.7] \n",
        "\n",
        "olan tek neuronlu bir neural network oluştur.\n",
        "\n",
        "Weight ve bias değeri random olacak. Bunun için SEED = 1 olacak\n",
        "\n",
        "Aktivasyon fonksiyonu ise sigmoid. Formülü:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Önce kendi sigmoid fonksiyonunuzu yazın ve sonuç alın.\n",
        "\n",
        "Sonra pytorch'un sigmoid fonksiyonunu kullanarak sonucu teyid edin.\n",
        "\n",
        "Geçen haftaki kodları kullanacaksınız. Geçen hafta görmediğiniz ve bu çalışmada kullanacağınız tek yeni kod:\n",
        "\n",
        "torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "qcqCkLgXXPU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([0.1, 0.3, 0.7]).reshape(-1, 1)\n",
        "\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uW6FZtIXOK8",
        "outputId": "e7cf5085-0148-4892-ecb3-a0d4e2f35e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1000],\n",
              "        [0.3000],\n",
              "        [0.7000]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olması gereken:\n",
        "```\n",
        "tensor([[0.1000],\n",
        "        [0.3000],\n",
        "        [0.7000]])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Yx0wpd04XZpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1) # Bunu da görmemiştiniz, o yüzden hazır veriyorum\n",
        "\n",
        "weight = torch.randn(1, 3)\n",
        "\n",
        "bias = torch.randn(1, 1)\n",
        "\n",
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH3Ht4JkXWPl",
        "outputId": "ee17a9ad-b98d-4c3e-af01-9e6beb58d453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.6614, 0.2669, 0.0617]]), tensor([[0.6213]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktivasyon fonksiyonunu yaz:"
      ],
      "metadata": {
        "id": "Zad2W6quXlfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_activation(x):\n",
        "\n",
        "  return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "yw3EEYhnXfBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fonksiyona giren"
      ],
      "metadata": {
        "id": "v4_z20YfXrnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = torch.matmul(weight, x) + bias\n",
        "\n",
        "tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE7K9zjAXrTh",
        "outputId": "c8bd4bf8-8a52-40c0-afd7-5802ff19303f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8107]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aktivasyon sonucu"
      ],
      "metadata": {
        "id": "7Iy2P5qhX0Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_activation(tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxDYLBrrX1Kf",
        "outputId": "e9122bb2-4ae7-4378-f0f6-c8132c28a6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6923]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hazır aktivasyon fonksiyonu ne verir?"
      ],
      "metadata": {
        "id": "T2jwMa2dX16Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sigmoid(tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Ri9qPCX4TQ",
        "outputId": "328259eb-5cff-47f8-b78e-5f5a6bbd44fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6923]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kendiniz yapın:\n",
        "\n",
        "yukarıdaki tek adet neuron ile input arasına bir adet 2 neuronlu hidden layer ekleyip, ona da random weight ve bias değerleri atayıp, bu neuronlara da sigmoid aktivasyonu ekleyip, az önce oluşturduğunuz aynı output neuronundan sonuç değeri alın.\n",
        "\n",
        "Seed = 1 için çıkacak değer tensor([[0.4632]])"
      ],
      "metadata": {
        "id": "6roJbu4QYHnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DERIVATIVE (Türev)!!!\n",
        "\n"
      ],
      "metadata": {
        "id": "bmO_SEeAELlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, we *train* models, updating them successively\n",
        "so that they get better and better as they see more and more data.\n",
        "Usually, getting better means minimizing a *loss function*,\n",
        "a score that answers the question \"how *bad* is our model?\"\n",
        "This question is more subtle than it appears.\n",
        "Ultimately, what we really care about\n",
        "is producing a model that performs well on data\n",
        "that we have never seen before.\n",
        "But we can only fit the model to data that we can actually see.\n",
        "Thus we can decompose the task of fitting models into two key concerns:\n",
        "(i) *optimization*: the process of fitting our models to observed data;\n",
        "(ii) *generalization*: the mathematical principles and practitioners' wisdom\n",
        "that guide as to how to produce models whose validity extends\n",
        "beyond the exact set of data examples used to train them.\n",
        "\n",
        "To help you understand\n",
        "optimization problems and methods in later chapters,\n",
        "here we give a very brief primer on differential calculus\n",
        "that is commonly used in deep learning.\n",
        "\n",
        "## Derivatives and Differentiation\n",
        "\n",
        "We begin by addressing the calculation of derivatives,\n",
        "a crucial step in nearly all deep learning optimization algorithms.\n",
        "In deep learning, we typically choose loss functions\n",
        "that are differentiable with respect to our model's parameters.\n",
        "Put simply, this means that for each parameter,\n",
        "we can determine how rapidly the loss would increase or decrease,\n",
        "were we to *increase* or *decrease* that parameter\n",
        "by an infinitesimally small amount.\n",
        "\n",
        "Suppose that we have a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
        "whose input and output are both scalars.\n",
        "[**The *derivative* of $f$ is defined as**]\n",
        "\n",
        "\n",
        "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$$**)\n",
        ":eqlabel:`eq_derivative`\n",
        "\n",
        "if this limit exists.\n",
        "If $f'(a)$ exists,\n",
        "$f$ is said to be *differentiable* at $a$.\n",
        "If $f$ is differentiable at every number of an interval,\n",
        "then this function is differentiable on this interval.\n",
        "We can interpret the derivative $f'(x)$ in :eqref:`eq_derivative`\n",
        "as the *instantaneous* rate of change of $f(x)$\n",
        "with respect to $x$.\n",
        "The so-called instantaneous rate of change is based on\n",
        "the variation $h$ in $x$, which approaches $0$.\n",
        "\n",
        "To illustrate derivatives,\n",
        "let us experiment with an example.\n",
        "(**Define $u = f(x) = 3x^2-4x$.**)\n"
      ],
      "metadata": {
        "id": "w2O8yMd7ozI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ],
      "metadata": {
        "id": "NhzPMtYqETns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**By setting $x=1$ and letting $h$ approach $0$,\n",
        "the numerical result of $\\frac{f(x+h) - f(x)}{h}$**]\n",
        "in :eqref:`eq_derivative`\n",
        "(**approaches $2$.**)\n",
        "Though this experiment is not a mathematical proof,\n",
        "we will see later that the derivative $u'$ is $2$ when $x=1$.\n"
      ],
      "metadata": {
        "id": "xo2Kt9ZiE1Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_lim(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "h = 0.1\n",
        "for i in range(5):\n",
        "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
        "    h *= 0.1"
      ],
      "metadata": {
        "id": "ra9flM70Eu2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd946608-977f-45f7-db07-acc2fd597b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h=0.10000, numerical limit=2.30000\n",
            "h=0.01000, numerical limit=2.03000\n",
            "h=0.00100, numerical limit=2.00300\n",
            "h=0.00010, numerical limit=2.00030\n",
            "h=0.00001, numerical limit=2.00003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us familiarize ourselves with a few equivalent notations for derivatives.\n",
        "Given $y = f(x)$, where $x$ and $y$ are the independent variable and the dependent variable of the function $f$, respectively. The following expressions are equivalent:\n",
        "\n",
        "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
        "\n",
        "where symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators* that indicate operation of *differentiation*.\n",
        "We can use the following rules to differentiate common functions:\n",
        "\n",
        "* $DC = 0$ ($C$ is a constant),\n",
        "* $Dx^n = nx^{n-1}$ (the *power rule*, $n$ is any real number),\n",
        "* $De^x = e^x$,\n",
        "* $D\\ln(x) = 1/x.$\n",
        "\n",
        "To differentiate a function that is formed from a few simpler functions such as the above common functions,\n",
        "the following rules can be handy for us.\n",
        "Suppose that functions $f$ and $g$ are both differentiable and $C$ is a constant,\n",
        "we have the *constant multiple rule*\n",
        "\n",
        "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
        "\n",
        "the *sum rule*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
        "\n",
        "the *product rule*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
        "\n",
        "and the *quotient rule*\n",
        "\n",
        "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
        "\n",
        "Now we can apply a few of the above rules to find\n",
        "$u' = f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$.\n",
        "Thus, by setting $x = 1$, we have $u' = 2$:\n",
        "this is supported by our earlier experiment in this section\n",
        "where the numerical result approaches $2$.\n",
        "This derivative is also the slope of the tangent line\n",
        "to the curve $u = f(x)$ when $x = 1$."
      ],
      "metadata": {
        "id": "S8MtR_sQE3tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partial Derivatives\n",
        "\n",
        "So far we have dealt with the differentiation of functions of just one variable.\n",
        "In deep learning, functions often depend on *many* variables.\n",
        "Thus, we need to extend the ideas of differentiation to these *multivariate* functions.\n",
        "\n",
        "\n",
        "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. The *partial derivative* of $y$ with respect to its $i^\\mathrm{th}$  parameter $x_i$ is\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
        "\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$, we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants and calculate the derivative of $y$ with respect to $x_i$.\n",
        "For notation of partial derivatives, the following are equivalent:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
        "\n",
        "\n",
        "## Gradients\n",
        ":label:`subsec_calculus-grad`\n",
        "\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the *gradient* vector of the function.\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an $n$-dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is a vector of $n$ partial derivatives:\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
        "\n",
        "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$ when there is no ambiguity.\n",
        "\n",
        "Let $\\mathbf{x}$ be an $n$-dimensional vector, the following rules are often used when differentiating multivariate functions:\n",
        "\n",
        "* For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$,\n",
        "* For all  $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$,\n",
        "* For all  $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$,\n",
        "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$.\n",
        "\n",
        "Similarly, for any matrix $\\mathbf{X}$, we have $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$. As we will see later, gradients are useful for designing optimization algorithms in deep learning.\n",
        "\n",
        "\n",
        "## Chain Rule\n",
        "\n",
        "However, such gradients can be hard to find.\n",
        "This is because multivariate functions in deep learning are often *composite*,\n",
        "so we may not apply any of the aforementioned rules to differentiate these functions.\n",
        "Fortunately, the *chain rule* enables us to differentiate composite functions.\n",
        "\n",
        "Let us first consider functions of a single variable.\n",
        "Suppose that functions $y=f(u)$ and $u=g(x)$ are both differentiable, then the chain rule states that\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
        "\n",
        "Now let us turn our attention to a more general scenario\n",
        "where functions have an arbitrary number of variables.\n",
        "Suppose that the differentiable function $y$ has variables\n",
        "$u_1, u_2, \\ldots, u_m$, where each differentiable function $u_i$\n",
        "has variables $x_1, x_2, \\ldots, x_n$.\n",
        "Note that $y$ is a function of $x_1, x_2, \\ldots, x_n$.\n",
        "Then the chain rule gives\n",
        "\n",
        "$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n",
        "\n",
        "for any $i = 1, 2, \\ldots, n$."
      ],
      "metadata": {
        "id": "RUsrP81mFRjN"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bmO_SEeAELlW"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}