{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "XXXwOtndG7Id"
      },
      "source": [
        "(**To start, we import `torch`. Note that though it's called PyTorch, we should\n",
        "import `torch` instead of `pytorch`.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "id": "pAryy4CWG7Ie"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 7,
        "id": "wVQywCEMG7Ie"
      },
      "source": [
        "[**A tensor represents a (possibly multi-dimensional) array of numerical values.**]\n",
        "With one axis, a tensor is called a *vector*.\n",
        "With two axes, a tensor is called a *matrix*.\n",
        "With $k > 2$ axes, we drop the specialized names\n",
        "and just refer to the object as a $k^\\mathrm{th}$ *order tensor*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "pytorch"
        ],
        "id": "tQvtWgMbG7Ie"
      },
      "source": [
        "PyTorch provides a variety of functions \n",
        "for creating new tensors \n",
        "prepopulated with values. \n",
        "For example, by invoking `arange(n)`,\n",
        "we can create a vector of evenly spaced values,\n",
        "starting at 0 (included) \n",
        "and ending at `n` (not included).\n",
        "By default, the interval size is $1$.\n",
        "Unless otherwise specified, \n",
        "new tensors are stored in main memory \n",
        "and designated for CPU-based computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "RMHRTpxTG7If",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7fbea4-88fc-458f-f34d-159235789523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "x = torch.arange(12, dtype=torch.float32)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "BnZto0U_G7If"
      },
      "source": [
        "(**We can access a tensor's *shape***) (~~and the total number of elements~~) (the length along each axis)\n",
        "by inspecting its `shape` property.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "QnAxZmIGG7Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5697bf39-458e-4d43-b149-ed322e60722b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "0vkIJf1jG7Ig"
      },
      "source": [
        "If we just want to know the total number of elements in a tensor,\n",
        "i.e., the product of all of the shape elements,\n",
        "we can inspect its size.\n",
        "Because we are dealing with a vector here,\n",
        "the single element of its `shape` is identical to its size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "6OsWnRrSG7Ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad6df69-4449-4997-bbef-77fd8de7c42e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "yaPkLz24G7Ih"
      },
      "source": [
        "To [**change the shape of a tensor without altering\n",
        "either the number of elements or their values**],\n",
        "we can invoke the `reshape` function.\n",
        "For example, we can transform our tensor, `x`,\n",
        "from a row vector with shape (12,) to a matrix with shape (3, 4).\n",
        "This new tensor contains the exact same values,\n",
        "but views them as a matrix organized as 3 rows and 4 columns.\n",
        "To reiterate, although the shape has changed,\n",
        "the elements have not.\n",
        "Note that the size is unaltered by reshaping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "niK5BimlG7Ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd73a9f-12a7-400e-d65e-11a5f2a94f98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X = x.reshape(3, 4)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "id": "sQTdXN_tG7Ih"
      },
      "source": [
        "Reshaping by manually specifying every dimension is unnecessary.\n",
        "If our target shape is a matrix with shape (height, width),\n",
        "then after we know the width, the height is given implicitly.\n",
        "Why should we have to perform the division ourselves?\n",
        "In the example above, to get a matrix with 3 rows,\n",
        "we specified both that it should have 3 rows and 4 columns.\n",
        "Fortunately, tensors can automatically work out one dimension given the rest.\n",
        "We invoke this capability by placing `-1` for the dimension\n",
        "that we would like tensors to automatically infer.\n",
        "In our case, instead of calling `x.reshape(3, 4)`,\n",
        "we could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`.\n",
        "\n",
        "Typically, we will want our matrices initialized\n",
        "either with zeros, ones, some other constants,\n",
        "or numbers randomly sampled from a specific distribution.\n",
        "[**We can create a tensor representing a tensor with all elements\n",
        "set to 0**] (~~or 1~~)\n",
        "and a shape of (2, 3, 4) as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "UDPdP-ntG7Ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7606db1-dc78-4831-fb14-74992fa49b7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "torch.zeros((2, 3, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 27,
        "id": "8KLX4fEKG7Ii"
      },
      "source": [
        "Similarly, we can create tensors with each element set to 1 as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 29,
        "tab": [
          "pytorch"
        ],
        "id": "IFzu-5VWG7Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d96119-0d83-4226-c1e3-d32c5566bb0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.ones((2, 3, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 31,
        "id": "oxW1MaTwG7Ii"
      },
      "source": [
        "Often, we want to [**randomly sample the values\n",
        "for each element in a tensor**]\n",
        "from some probability distribution.\n",
        "For example, when we construct arrays to serve\n",
        "as parameters in a neural network, we will\n",
        "typically initialize their values randomly.\n",
        "The following snippet creates a tensor with shape (3, 4).\n",
        "Each of its elements is randomly sampled\n",
        "from a standard Gaussian (normal) distribution\n",
        "with a mean of 0 and a standard deviation of 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 33,
        "tab": [
          "pytorch"
        ],
        "id": "xJULjhhgG7Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7c3811-4bf8-4b08-ef17-e630af911799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4024, -0.4115, -0.0563,  0.9234],\n",
              "        [ 0.5645,  0.6611,  1.0784, -0.9050],\n",
              "        [-0.6994, -0.0418,  1.5507,  0.8256]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "torch.randn(3, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 35,
        "id": "coQoeUDZG7Ii"
      },
      "source": [
        "We can also [**specify the exact values for each element**] in the desired tensor\n",
        "by supplying a Python list (or list of lists) containing the numerical values.\n",
        "Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "8vn4ZCvlG7Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89ff0d2-deff-4d89-bdd8-e09a44ddd053"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 39,
        "id": "JokaLoE0G7Ii"
      },
      "source": [
        "## Operations\n",
        "\n",
        "This book is not about software engineering.\n",
        "Our interests are not limited to simply\n",
        "reading and writing data from/to arrays.\n",
        "We want to perform mathematical operations on those arrays.\n",
        "Some of the simplest and most useful operations\n",
        "are the *elementwise* operations.\n",
        "These apply a standard scalar operation\n",
        "to each element of an array.\n",
        "For functions that take two arrays as inputs,\n",
        "elementwise operations apply some standard binary operator\n",
        "on each pair of corresponding elements from the two arrays.\n",
        "We can create an elementwise function from any function\n",
        "that maps from a scalar to a scalar.\n",
        "\n",
        "In mathematical notation, we would denote such\n",
        "a *unary* scalar operator (taking one input)\n",
        "by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
        "This just means that the function is mapping\n",
        "from any real number ($\\mathbb{R}$) onto another.\n",
        "Likewise, we denote a *binary* scalar operator\n",
        "(taking two real inputs, and yielding one output)\n",
        "by the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
        "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*,\n",
        "and a binary operator $f$, we can produce a vector\n",
        "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
        "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
        "where $c_i, u_i$, and $v_i$ are the $i^\\mathrm{th}$ elements\n",
        "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
        "Here, we produced the vector-valued\n",
        "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
        "by *lifting* the scalar function to an elementwise vector operation.\n",
        "\n",
        "The common standard arithmetic operators\n",
        "(`+`, `-`, `*`, `/`, and `**`)\n",
        "have all been *lifted* to elementwise operations\n",
        "for any identically-shaped tensors of arbitrary shape.\n",
        "We can call elementwise operations on any two tensors of the same shape.\n",
        "In the following example, we use commas to formulate a 5-element tuple,\n",
        "where each element is the result of an elementwise operation.\n",
        "\n",
        "### Operations\n",
        "\n",
        "[**The common standard arithmetic operators\n",
        "(`+`, `-`, `*`, `/`, and `**`)\n",
        "have all been *lifted* to elementwise operations.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 41,
        "tab": [
          "pytorch"
        ],
        "id": "scAxorFgG7Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804a1248-bd39-4eb8-a5bf-3292dc4989c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 43,
        "id": "2NfCsseaG7Ij"
      },
      "source": [
        "Many (**more operations can be applied elementwise**),\n",
        "including unary operators like exponentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 45,
        "tab": [
          "pytorch"
        ],
        "id": "xN-PdzmIG7Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04ae17c-f9dc-4a53-f7a1-996b9fe4fb7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "torch.exp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 47,
        "id": "YoCTonnUG7Ij"
      },
      "source": [
        "In addition to elementwise computations,\n",
        "we can also perform linear algebra operations,\n",
        "including vector dot products and matrix multiplication.\n",
        "We will explain the crucial bits of linear algebra\n",
        "(with no assumed prior knowledge) in :numref:`sec_linear-algebra`.\n",
        "\n",
        "We can also [***concatenate* multiple tensors together,**]\n",
        "stacking them end-to-end to form a larger tensor.\n",
        "We just need to provide a list of tensors\n",
        "and tell the system along which axis to concatenate.\n",
        "The example below shows what happens when we concatenate\n",
        "two matrices along rows (axis 0, the first element of the shape)\n",
        "vs. columns (axis 1, the second element of the shape).\n",
        "We can see that the first output tensor's axis-0 length ($6$)\n",
        "is the sum of the two input tensors' axis-0 lengths ($3 + 3$);\n",
        "while the second output tensor's axis-1 length ($8$)\n",
        "is the sum of the two input tensors' axis-1 lengths ($4 + 4$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 49,
        "tab": [
          "pytorch"
        ],
        "id": "sOp4f64vG7Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f7086b-a5ed-4c18-ec60-a275a9144f4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]),\n",
              " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 51,
        "id": "xPd2dkePG7Ij"
      },
      "source": [
        "Sometimes, we want to [**construct a binary tensor via *logical statements*.**]\n",
        "Take `X == Y` as an example.\n",
        "For each position, if `X` and `Y` are equal at that position,\n",
        "the corresponding entry in the new tensor takes a value of 1,\n",
        "meaning that the logical statement `X == Y` is true at that position;\n",
        "otherwise that position takes 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 52,
        "tab": [
          "pytorch"
        ],
        "id": "-7W-S0K6G7Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234cec5b-20c1-4df0-b930-597d241093c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True, False,  True],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X == Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 53,
        "id": "Id3C3MKJG7Ij"
      },
      "source": [
        "[**Summing all the elements in the tensor**] yields a tensor with only one element.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 54,
        "tab": [
          "pytorch"
        ],
        "id": "xR122wYqG7Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81664311-73e9-4a0d-8388-3b90352dd014"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 56,
        "id": "gROg03I2G7Ik"
      },
      "source": [
        "## Broadcasting Mechanism\n",
        ":label:`subsec_broadcasting`\n",
        "\n",
        "In the above section, we saw how to perform elementwise operations\n",
        "on two tensors of the same shape. Under certain conditions,\n",
        "even when shapes differ, we can still [**perform elementwise operations\n",
        "by invoking the *broadcasting mechanism*.**]\n",
        "This mechanism works in the following way:\n",
        "First, expand one or both arrays\n",
        "by copying elements appropriately\n",
        "so that after this transformation,\n",
        "the two tensors have the same shape.\n",
        "Second, carry out the elementwise operations\n",
        "on the resulting arrays.\n",
        "\n",
        "In most cases, we broadcast along an axis where an array\n",
        "initially only has length 1, such as in the following example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 58,
        "tab": [
          "pytorch"
        ],
        "id": "VtYHHWPjG7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8c88e2-a7d6-439b-b27f-bbbd726fe3af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]), tensor([[0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 60,
        "id": "RbnPvZGiG7Ik"
      },
      "source": [
        "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices respectively,\n",
        "their shapes do not match up if we want to add them.\n",
        "We *broadcast* the entries of both matrices into a larger $3\\times2$ matrix as follows:\n",
        "for matrix `a` it replicates the columns\n",
        "and for matrix `b` it replicates the rows\n",
        "before adding up both elementwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 61,
        "tab": [
          "pytorch"
        ],
        "id": "Ezoaj4BZG7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25153eb1-8bca-41da-e51d-6bc7082dfb0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "a + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 62,
        "id": "XYY7wP2aG7Ik"
      },
      "source": [
        "## Indexing and Slicing\n",
        "\n",
        "Just as in any other Python array, elements in a tensor can be accessed by index.\n",
        "As in any Python array, the first element has index 0\n",
        "and ranges are specified to include the first but *before* the last element.\n",
        "As in standard Python lists, we can access elements\n",
        "according to their relative position to the end of the list\n",
        "by using negative indices.\n",
        "\n",
        "Thus, [**`[-1]` selects the last element and `[1:3]`\n",
        "selects the second and the third elements**] as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 63,
        "tab": [
          "pytorch"
        ],
        "id": "3KnJfOdGG7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06dcd708-1906-4d84-8202-b2f75e5e65e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "X[-1], X[1:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 64,
        "tab": [
          "pytorch"
        ],
        "id": "5NlAkYXVG7Ik"
      },
      "source": [
        "Beyond reading, (**we can also write elements of a matrix by specifying indices.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 66,
        "tab": [
          "pytorch"
        ],
        "id": "jspMJQWQG7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f14c40-a865-4699-a32a-2d9142a084d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  9.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X[1, 2] = 9\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 68,
        "id": "4wmIeRikG7Ik"
      },
      "source": [
        "If we want [**to assign multiple elements the same value,\n",
        "we simply index all of them and then assign them the value.**]\n",
        "For instance, `[0:2, :]` accesses the first and second rows,\n",
        "where `:` takes all the elements along axis 1 (column).\n",
        "While we discussed indexing for matrices,\n",
        "this obviously also works for vectors\n",
        "and for tensors of more than 2 dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 69,
        "tab": [
          "pytorch"
        ],
        "id": "A17VmshzG7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f54209b-9eb1-4d17-a7c2-c369fc06c394"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12., 12.],\n",
              "        [12., 12., 12., 12.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X[0:2, :] = 12\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 83,
        "id": "TJVhv2VRG7Il"
      },
      "source": [
        "## Conversion to Other Python Objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 85,
        "tab": [
          "pytorch"
        ],
        "id": "-Cz3ieCxG7Il"
      },
      "source": [
        "[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\n",
        "The torch Tensor and numpy array will share their underlying memory\n",
        "locations, and changing one through an in-place operation will also\n",
        "change the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 87,
        "tab": [
          "pytorch"
        ],
        "id": "G0_zxfm8G7Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791a0f9c-b2e7-4dd5-8de1-c1a3c0694e3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "A = X.numpy()\n",
        "B = torch.from_numpy(A)\n",
        "type(A), type(B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 89,
        "id": "kB8nRYzSG7Il"
      },
      "source": [
        "To (**convert a size-1 tensor to a Python scalar**),\n",
        "we can invoke the `item` function or Python's built-in functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 91,
        "tab": [
          "pytorch"
        ],
        "id": "a7Tb9Rv1G7Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e400eb6-4463-4a65-e2dd-b59d7c8d11f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3.5000]), 3.5, 3.5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "a = torch.tensor([3.5])\n",
        "a, a.item(), float(a), int(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        ":label:`sec_pandas`\n",
        "\n",
        "So far we have introduced a variety of techniques for manipulating data that are already stored in tensors.\n",
        "To apply deep learning to solving real-world problems,\n",
        "we often begin with preprocessing raw data, rather than those nicely prepared data in the tensor format.\n",
        "Among popular data analytic tools in Python, the `pandas` package is commonly used.\n",
        "Like many other extension packages in the vast ecosystem of Python,\n",
        "`pandas` can work together with tensors.\n",
        "So, we will briefly walk through steps for preprocessing raw data with `pandas`\n",
        "and converting them into the tensor format.\n",
        "We will cover more data preprocessing techniques in later chapters.\n",
        "\n",
        "## Reading the Dataset\n",
        "\n",
        "As an example,\n",
        "we begin by (**creating an artificial dataset that is stored in a\n",
        "csv (comma-separated values) file**)\n",
        "`../data/house_tiny.csv`. Data stored in other\n",
        "formats may be processed in similar ways.\n",
        "\n",
        "Below we write the dataset row by row into a csv file.\n"
      ],
      "metadata": {
        "id": "HmKiAdMCCNlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
        "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "ee10_CMRCSm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If pandas is not installed, just uncomment the following line:\n",
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "tp3HW5CKCcMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c94e5c8-050b-4fcf-85fb-fb4fcc73d326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley   Price\n",
            "0       NaN  Pave  127500\n",
            "1       2.0   NaN  106000\n",
            "2       4.0   NaN  178100\n",
            "3       NaN   NaN  140000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "b9PUS6ahBEvQ",
        "outputId": "c46911e3-6f10-404b-ae72-089385fcf157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NumRooms Alley   Price\n",
              "0       NaN  Pave  127500\n",
              "1       2.0   NaN  106000\n",
              "2       4.0   NaN  178100\n",
              "3       NaN   NaN  140000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8f188f3-c2cf-46f8-bc3d-83985b9ef962\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumRooms</th>\n",
              "      <th>Alley</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Pave</td>\n",
              "      <td>127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>178100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8f188f3-c2cf-46f8-bc3d-83985b9ef962')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8f188f3-c2cf-46f8-bc3d-83985b9ef962 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8f188f3-c2cf-46f8-bc3d-83985b9ef962');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Price\"].mean()"
      ],
      "metadata": {
        "id": "GUpGmWfPChCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96518e9-77c6-411d-b082-3eba169d659d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137900.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnRQigepCi_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Data\n",
        "\n",
        "Note that \"NaN\" entries are missing values.\n",
        "To handle missing data, typical methods include *imputation* and *deletion*,\n",
        "where imputation replaces missing values with substituted ones,\n",
        "while deletion ignores missing values. Here we will consider imputation.\n",
        "\n",
        "By integer-location based indexing (`iloc`), we split `data` into `inputs` and `outputs`,\n",
        "where the former takes the first two columns while the latter only keeps the last column.\n",
        "For numerical values in `inputs` that are missing,\n",
        "we [**replace the \"NaN\" entries with the mean value of the same column.**]\n"
      ],
      "metadata": {
        "id": "w31ZOsuACkys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "2XkBTQJ5ClWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e159620-3e00-427e-fa8c-ae1382ef5504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley\n",
            "0       3.0  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       3.0   NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7f3bcd4dc11f>:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  inputs = inputs.fillna(inputs.mean())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**For categorical or discrete values in `inputs`, we consider \"NaN\" as a category.**]\n",
        "Since the \"Alley\" column only takes two types of categorical values \"Pave\" and \"NaN\",\n",
        "`pandas` can automatically convert this column to two columns \"Alley_Pave\" and \"Alley_nan\".\n",
        "A row whose alley type is \"Pave\" will set values of \"Alley_Pave\" and \"Alley_nan\" to 1 and 0.\n",
        "A row with a missing alley type will set their values to 0 and 1.\n"
      ],
      "metadata": {
        "id": "KJQnPLNICpoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "3lvG0t64CnXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454689a3-d38d-47e7-c6c8-47b9c83e495f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0           1          0\n",
            "1       2.0           0          1\n",
            "2       4.0           0          1\n",
            "3       3.0           0          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion to the Tensor Format\n",
        "\n",
        "Now that [**all the entries in `inputs` and `outputs` are numerical, they can be converted to the tensor format.**]\n",
        "Once data are in this format, they can be further manipulated with those tensor functionalities that we have introduced in :numref:`sec_ndarray`.\n"
      ],
      "metadata": {
        "id": "DE-wu6eXCse4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
        "X, y"
      ],
      "metadata": {
        "id": "EPK9Er3oCu3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra\n",
        ":label:`sec_linear-algebra`\n",
        "\n",
        "\n",
        "Now that you can store and manipulate data,\n",
        "let us briefly review the subset of basic linear algebra\n",
        "that you will need to understand and implement\n",
        "most of models covered in this book.\n",
        "Below, we introduce the basic mathematical objects, arithmetic,\n",
        "and operations in linear algebra,\n",
        "expressing each of them through mathematical notation\n",
        "and the corresponding implementation in code.\n"
      ],
      "metadata": {
        "id": "scsTpiKfC6C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "x + y, x * y, x / y, x**y"
      ],
      "metadata": {
        "id": "8xJC2KCuCw9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectors\n",
        "\n",
        "[**You can think of a vector as simply a list of scalar values.**]\n",
        "We call these values the *elements* (*entries* or *components*) of the vector.\n",
        "When our vectors represent examples from our dataset,\n",
        "their values hold some real-world significance.\n",
        "For example, if we were training a model to predict\n",
        "the risk that a loan defaults,\n",
        "we might associate each applicant with a vector\n",
        "whose components correspond to their income,\n",
        "length of employment, number of previous defaults, and other factors.\n",
        "If we were studying the risk of heart attacks hospital patients potentially face,\n",
        "we might represent each patient by a vector\n",
        "whose components capture their most recent vital signs,\n",
        "cholesterol levels, minutes of exercise per day, etc.\n",
        "In math notation, we will usually denote vectors as bold-faced,\n",
        "lower-cased letters (e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n",
        "\n",
        "We work with vectors via one-dimensional tensors.\n",
        "In general tensors can have arbitrary lengths,\n",
        "subject to the memory limits of your machine.\n"
      ],
      "metadata": {
        "id": "akOPWedkC9YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4)\n",
        "x"
      ],
      "metadata": {
        "id": "lgaKgFjeC_1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can refer to any element of a vector by using a subscript.\n",
        "For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$.\n",
        "Note that the element $x_i$ is a scalar,\n",
        "so we do not bold-face the font when referring to it.\n",
        "Extensive literature considers column vectors to be the default\n",
        "orientation of vectors, so does this book.\n",
        "In math, a vector $\\mathbf{x}$ can be written as\n",
        "\n",
        "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
        ":eqlabel:`eq_vec_def`\n",
        "\n",
        "\n",
        "where $x_1, \\ldots, x_n$ are elements of the vector.\n",
        "In code,\n",
        "we (**access any element by indexing into the tensor.**)\n"
      ],
      "metadata": {
        "id": "XHK-vKQQDByB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "id": "lAgL3tR-DEaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length, Dimensionality, and Shape\n",
        "\n",
        "Let us revisit some concepts from :numref:`sec_ndarray`.\n",
        "A vector is just an array of numbers.\n",
        "And just as every array has a length, so does every vector.\n",
        "In math notation, if we want to say that a vector $\\mathbf{x}$\n",
        "consists of $n$ real-valued scalars,\n",
        "we can express this as $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
        "The length of a vector is commonly called the *dimension* of the vector.\n",
        "\n",
        "As with an ordinary Python array,\n",
        "we [**can access the length of a tensor**]\n",
        "by calling Python's built-in `len()` function.\n"
      ],
      "metadata": {
        "id": "HGkrrO7uDGV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "id": "wH4m5eptDIym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "nQ9MM_akDMr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the word \"dimension\" tends to get overloaded\n",
        "in these contexts and this tends to confuse people.\n",
        "To clarify, we use the dimensionality of a *vector* or an *axis*\n",
        "to refer to its length, i.e., the number of elements of a vector or an axis.\n",
        "However, we use the dimensionality of a tensor\n",
        "to refer to the number of axes that a tensor has.\n",
        "In this sense, the dimensionality of some axis of a tensor\n",
        "will be the length of that axis.\n",
        "\n",
        "\n",
        "## Matrices\n",
        "\n",
        "Just as vectors generalize scalars from order zero to order one,\n",
        "matrices generalize vectors from order one to order two.\n",
        "Matrices, which we will typically denote with bold-faced, capital letters\n",
        "(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$),\n",
        "are represented in code as tensors with two axes.\n",
        "\n",
        "In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
        "to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars.\n",
        "Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table,\n",
        "where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n",
        "\n",
        "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
        ":eqlabel:`eq_matrix_def`\n",
        "\n",
        "\n",
        "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$\n",
        "is ($m$, $n$) or $m \\times n$.\n",
        "Specifically, when a matrix has the same number of rows and columns,\n",
        "its shape becomes a square; thus, it is called a *square matrix*.\n",
        "\n",
        "We can [**create an $m \\times n$ matrix**]\n",
        "by specifying a shape with two components $m$ and $n$\n",
        "when calling any of our favorite functions for instantiating a tensor.\n"
      ],
      "metadata": {
        "id": "u9aa7TY_DKWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20).reshape(5, 4)\n",
        "A"
      ],
      "metadata": {
        "id": "3CcVNitTDQEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the scalar element $a_{ij}$ of a matrix $\\mathbf{A}$ in :eqref:`eq_matrix_def`\n",
        "by specifying the indices for the row ($i$) and column ($j$),\n",
        "such as $[\\mathbf{A}]_{ij}$.\n",
        "When the scalar elements of a matrix $\\mathbf{A}$, such as in :eqref:`eq_matrix_def`, are not given,\n",
        "we may simply use the lower-case letter of the matrix $\\mathbf{A}$ with the index subscript, $a_{ij}$,\n",
        "to refer to $[\\mathbf{A}]_{ij}$.\n",
        "To keep notation simple, commas are inserted to separate indices only when necessary,\n",
        "such as $a_{2, 3j}$ and $[\\mathbf{A}]_{2i-1, 3}$.\n",
        "\n",
        "\n",
        "Sometimes, we want to flip the axes.\n",
        "When we exchange a matrix's rows and columns,\n",
        "the result is called the *transpose* of the matrix.\n",
        "Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$\n",
        "and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for any $i$ and $j$.\n",
        "Thus, the transpose of $\\mathbf{A}$ in :eqref:`eq_matrix_def` is\n",
        "a $n \\times m$ matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{bmatrix}\n",
        "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
        "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
        "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
        "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Now we access a (**matrix's transpose**) in code.\n"
      ],
      "metadata": {
        "id": "0B4JqK4NDUg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.T"
      ],
      "metadata": {
        "id": "KjciUquVDVGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrices are useful data structures:\n",
        "they allow us to organize data that have different modalities of variation.\n",
        "For example, rows in our matrix might correspond to different houses (data examples),\n",
        "while columns might correspond to different attributes.\n",
        "This should sound familiar if you have ever used spreadsheet software or\n",
        "have read :numref:`sec_pandas`.\n",
        "Thus, although the default orientation of a single vector is a column vector,\n",
        "in a matrix that represents a tabular dataset,\n",
        "it is more conventional to treat each data example as a row vector in the matrix.\n",
        "And, as we will see in later chapters,\n",
        "this convention will enable common deep learning practices.\n",
        "For example, along the outermost axis of a tensor,\n",
        "we can access or enumerate minibatches of data examples,\n",
        "or just data examples if no minibatch exists.\n",
        "\n",
        "\n",
        "## Tensors\n",
        "\n",
        "Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes.\n",
        "[**Tensors**]\n",
        "(\"tensors\" in this subsection refer to algebraic objects)\n",
        "(**give us a generic way of describing $n$-dimensional arrays with an arbitrary number of axes.**)\n",
        "Vectors, for example, are first-order tensors, and matrices are second-order tensors.\n",
        "Tensors are denoted with capital letters of a special font face\n",
        "(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\n",
        "and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) is similar to that of matrices.\n",
        "\n",
        "Tensors will become more important when we start working with images,\n",
        " which arrive as $n$-dimensional arrays with 3 axes corresponding to the height, width, and a *channel* axis for stacking the color channels (red, green, and blue). For now, we will skip over higher order tensors and focus on the basics.\n"
      ],
      "metadata": {
        "id": "7viA7aPwDcBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "X"
      ],
      "metadata": {
        "id": "4YC28H_VDW9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
        "A, A + B"
      ],
      "metadata": {
        "id": "UaajkHSZDiCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "id": "3o6oY7JeDkXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "a + X, (a * X).shape"
      ],
      "metadata": {
        "id": "Dm5XWsSADmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(axis=[0, 1])  # Same as `A.sum()`"
      ],
      "metadata": {
        "id": "saZyLi_oDrEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(), A.sum() / A.numel()"
      ],
      "metadata": {
        "id": "h_c-MEoWDr8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
      ],
      "metadata": {
        "id": "O9JfGJbKDxWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix-Matrix Multiplication\n",
        "\n",
        "If you have gotten the hang of dot products and matrix-vector products,\n",
        "then *matrix-matrix multiplication* should be straightforward.\n",
        "\n",
        "Say that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
        "\n",
        "$$\\mathbf{A}=\\begin{bmatrix}\n",
        " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
        " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
        "\\end{bmatrix},\\quad\n",
        "\\mathbf{B}=\\begin{bmatrix}\n",
        " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
        " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "\n",
        "Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$\n",
        "the row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$,\n",
        "and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$\n",
        "be the column vector from the $j^\\mathrm{th}$ column of the matrix $\\mathbf{B}$.\n",
        "To produce the matrix product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, it is easiest to think of $\\mathbf{A}$ in terms of its row vectors and $\\mathbf{B}$ in terms of its column vectors:\n",
        "\n",
        "$$\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_n \\\\\n",
        "\\end{bmatrix},\n",
        "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "Then the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is produced as we simply compute each element $c_{ij}$ as the dot product $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
        "\n",
        "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_n \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
        " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
        " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
        "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "[**We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix.**]\n",
        "In the following snippet, we perform matrix multiplication on `A` and `B`.\n",
        "Here,`A` is a matrix with 5 rows and 4 columns,\n",
        "and `B` is a matrix with 4 rows and 3 columns.\n",
        "After multiplication, we obtain a matrix with 5 rows and 3 columns.\n"
      ],
      "metadata": {
        "id": "42pk1QWxD4rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.ones(4, 3)\n",
        "torch.mm(A, B)"
      ],
      "metadata": {
        "id": "pJcTyXXrD5Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DERIVATIVE (Trev)!!!\n",
        "\n",
        "In deep learning, we *train* models, updating them successively\n",
        "so that they get better and better as they see more and more data.\n",
        "Usually, getting better means minimizing a *loss function*,\n",
        "a score that answers the question \"how *bad* is our model?\"\n",
        "This question is more subtle than it appears.\n",
        "Ultimately, what we really care about\n",
        "is producing a model that performs well on data\n",
        "that we have never seen before.\n",
        "But we can only fit the model to data that we can actually see.\n",
        "Thus we can decompose the task of fitting models into two key concerns:\n",
        "(i) *optimization*: the process of fitting our models to observed data;\n",
        "(ii) *generalization*: the mathematical principles and practitioners' wisdom\n",
        "that guide as to how to produce models whose validity extends\n",
        "beyond the exact set of data examples used to train them.\n",
        "\n",
        "To help you understand\n",
        "optimization problems and methods in later chapters,\n",
        "here we give a very brief primer on differential calculus\n",
        "that is commonly used in deep learning.\n",
        "\n",
        "## Derivatives and Differentiation\n",
        "\n",
        "We begin by addressing the calculation of derivatives,\n",
        "a crucial step in nearly all deep learning optimization algorithms.\n",
        "In deep learning, we typically choose loss functions\n",
        "that are differentiable with respect to our model's parameters.\n",
        "Put simply, this means that for each parameter,\n",
        "we can determine how rapidly the loss would increase or decrease,\n",
        "were we to *increase* or *decrease* that parameter\n",
        "by an infinitesimally small amount.\n",
        "\n",
        "Suppose that we have a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
        "whose input and output are both scalars.\n",
        "[**The *derivative* of $f$ is defined as**]\n",
        "\n",
        "\n",
        "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$$**)\n",
        ":eqlabel:`eq_derivative`\n",
        "\n",
        "if this limit exists.\n",
        "If $f'(a)$ exists,\n",
        "$f$ is said to be *differentiable* at $a$.\n",
        "If $f$ is differentiable at every number of an interval,\n",
        "then this function is differentiable on this interval.\n",
        "We can interpret the derivative $f'(x)$ in :eqref:`eq_derivative`\n",
        "as the *instantaneous* rate of change of $f(x)$\n",
        "with respect to $x$.\n",
        "The so-called instantaneous rate of change is based on\n",
        "the variation $h$ in $x$, which approaches $0$.\n",
        "\n",
        "To illustrate derivatives,\n",
        "let us experiment with an example.\n",
        "(**Define $u = f(x) = 3x^2-4x$.**)\n"
      ],
      "metadata": {
        "id": "bmO_SEeAELlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ],
      "metadata": {
        "id": "NhzPMtYqETns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**By setting $x=1$ and letting $h$ approach $0$,\n",
        "the numerical result of $\\frac{f(x+h) - f(x)}{h}$**]\n",
        "in :eqref:`eq_derivative`\n",
        "(**approaches $2$.**)\n",
        "Though this experiment is not a mathematical proof,\n",
        "we will see later that the derivative $u'$ is $2$ when $x=1$.\n"
      ],
      "metadata": {
        "id": "xo2Kt9ZiE1Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_lim(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "h = 0.1\n",
        "for i in range(5):\n",
        "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
        "    h *= 0.1"
      ],
      "metadata": {
        "id": "ra9flM70Eu2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us familiarize ourselves with a few equivalent notations for derivatives.\n",
        "Given $y = f(x)$, where $x$ and $y$ are the independent variable and the dependent variable of the function $f$, respectively. The following expressions are equivalent:\n",
        "\n",
        "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
        "\n",
        "where symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators* that indicate operation of *differentiation*.\n",
        "We can use the following rules to differentiate common functions:\n",
        "\n",
        "* $DC = 0$ ($C$ is a constant),\n",
        "* $Dx^n = nx^{n-1}$ (the *power rule*, $n$ is any real number),\n",
        "* $De^x = e^x$,\n",
        "* $D\\ln(x) = 1/x.$\n",
        "\n",
        "To differentiate a function that is formed from a few simpler functions such as the above common functions,\n",
        "the following rules can be handy for us.\n",
        "Suppose that functions $f$ and $g$ are both differentiable and $C$ is a constant,\n",
        "we have the *constant multiple rule*\n",
        "\n",
        "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
        "\n",
        "the *sum rule*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
        "\n",
        "the *product rule*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
        "\n",
        "and the *quotient rule*\n",
        "\n",
        "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
        "\n",
        "Now we can apply a few of the above rules to find\n",
        "$u' = f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$.\n",
        "Thus, by setting $x = 1$, we have $u' = 2$:\n",
        "this is supported by our earlier experiment in this section\n",
        "where the numerical result approaches $2$.\n",
        "This derivative is also the slope of the tangent line\n",
        "to the curve $u = f(x)$ when $x = 1$."
      ],
      "metadata": {
        "id": "S8MtR_sQE3tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partial Derivatives\n",
        "\n",
        "So far we have dealt with the differentiation of functions of just one variable.\n",
        "In deep learning, functions often depend on *many* variables.\n",
        "Thus, we need to extend the ideas of differentiation to these *multivariate* functions.\n",
        "\n",
        "\n",
        "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. The *partial derivative* of $y$ with respect to its $i^\\mathrm{th}$  parameter $x_i$ is\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
        "\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$, we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants and calculate the derivative of $y$ with respect to $x_i$.\n",
        "For notation of partial derivatives, the following are equivalent:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
        "\n",
        "\n",
        "## Gradients\n",
        ":label:`subsec_calculus-grad`\n",
        "\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the *gradient* vector of the function.\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an $n$-dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is a vector of $n$ partial derivatives:\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
        "\n",
        "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$ when there is no ambiguity.\n",
        "\n",
        "Let $\\mathbf{x}$ be an $n$-dimensional vector, the following rules are often used when differentiating multivariate functions:\n",
        "\n",
        "* For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$,\n",
        "* For all  $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$,\n",
        "* For all  $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$,\n",
        "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$.\n",
        "\n",
        "Similarly, for any matrix $\\mathbf{X}$, we have $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$. As we will see later, gradients are useful for designing optimization algorithms in deep learning.\n",
        "\n",
        "\n",
        "## Chain Rule\n",
        "\n",
        "However, such gradients can be hard to find.\n",
        "This is because multivariate functions in deep learning are often *composite*,\n",
        "so we may not apply any of the aforementioned rules to differentiate these functions.\n",
        "Fortunately, the *chain rule* enables us to differentiate composite functions.\n",
        "\n",
        "Let us first consider functions of a single variable.\n",
        "Suppose that functions $y=f(u)$ and $u=g(x)$ are both differentiable, then the chain rule states that\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
        "\n",
        "Now let us turn our attention to a more general scenario\n",
        "where functions have an arbitrary number of variables.\n",
        "Suppose that the differentiable function $y$ has variables\n",
        "$u_1, u_2, \\ldots, u_m$, where each differentiable function $u_i$\n",
        "has variables $x_1, x_2, \\ldots, x_n$.\n",
        "Note that $y$ is a function of $x_1, x_2, \\ldots, x_n$.\n",
        "Then the chain rule gives\n",
        "\n",
        "$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n",
        "\n",
        "for any $i = 1, 2, \\ldots, n$."
      ],
      "metadata": {
        "id": "RUsrP81mFRjN"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}